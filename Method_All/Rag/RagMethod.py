# åŠŸèƒ½ï¼šå®ç°ä¸ç§æœ‰çŸ¥è¯†åº“çš„äº¤äº’
# å…³é”®è¯ï¼šä¸ºäº†æ–¹ä¾¿ç†è§£å’Œè®°å¿†ï¼Œå°†è¯ä¸­å…³é”®çš„éƒ¨åˆ†æå–å‡ºæ¥ï¼Œä¸‹ä¸€æ¬¡çœ‹åˆ°è¿™ä¸€éƒ¨åˆ†èƒ½æœ‰å¤§è‡´çš„å°è±¡

# å…è®¸å’Œopenaiæ¨¡å‹å¯¹è¯
# å…³é”®è¯openai,chat
from langchain_openai import ChatOpenAI,OpenAIEmbeddings
# ç”¨äºè¯»å–æ–‡æ¡£/è·å–ç§æœ‰çŸ¥è¯†åº“   importéƒ¨åˆ†,åˆ†åˆ«æ˜¯å¤„ç†PDFå’Œå¤„ç†textæ–‡æœ¬
# å…³é”®è¯document(æ–‡æ¡£),PDF,Text
from langchain_community.document_loaders import PyPDFLoader,Docx2txtLoader,TextLoader
# å®ç°æ–‡æœ¬åˆ†å‰²  å¯¹æ–‡æœ¬çš„åˆ†æï¼Œæœ¬è´¨ä¸Šæ˜¯æŠŠæ–‡æœ¬æŒ‰ç…§ä¸€å®šçš„å¤§å°ï¼Œåˆ†å‰²æˆå¤šä¸ªæ®µè½
# åç»­çš„æ–‡æœ¬æŸ¥æ‰¾ï¼ŒæŸ¥æ‰¾çš„å¯¹è±¡æ˜¯åˆ†å‰²åçš„æ®µè½  æ®µè½ä¹Ÿæ˜¯ä¸ºäº†æ–¹ä¾¿å‘é‡åŒ–
# å…³é”®è¯split(åˆ†å‰²),character(æ®µè½)
from langchain.text_splitter import RecursiveCharacterTextSplitter
# æ–‡æœ¬è¿‡æ»¤å·¥å…·  é€šè¿‡ç®€åŒ–æ–‡æœ¬çš„å¤æ‚æ•°æ®ï¼Œåªä¿ç•™å…³é”®ä¿¡æ¯ï¼Œç›®çš„æ˜¯æ–¹ä¾¿å‘é‡åŒ–å’Œè®­ç»ƒ
# å…³é”®è¯vector(å‘é‡),complex(å¤æ‚)
from langchain_community.vectorstores.utils import filter_complex_metadata
# Chromaå‘é‡å­˜å‚¨åº“  å°†æ–‡æœ¬æ•°æ®å‘é‡åŒ–å¹¶å­˜å‚¨  
# Chromaå¯ä»¥å®Œæˆembeddingç”Ÿæˆçš„ä»»åŠ¡
# å…³é”®è¯Chroma
from langchain_community.vectorstores import Chroma
# æç¤ºæ¨¡æ¿åº“    ç›®çš„æ˜¯ç”Ÿæˆé€‚åˆç‰¹å®šä»»åŠ¡çš„æç¤ºè¯
# å…³é”®è¯prompt(æç¤º)
from langchain_core.prompts import ChatPromptTemplate
# å¯ä»¥è¿è¡Œçš„é€»è¾‘ç»„ä»¶ï¼Œå°†æ¨¡å‹å’Œå…¶ä»–æµç¨‹é›†æˆåœ¨ä¸€èµ·ã€‚å®ç°1+1
# å…³é”®è¯runnable(å¯è¿è¡Œ)
from langchain.schema.runnable import RunnablePassthrough
# å¤„ç†è§£æå™¨ï¼ŒæŠŠè¿”å›çš„ç»“æœå˜æˆå­—ç¬¦ä¸²
# å…³é”®è¯output(è¾“å‡º),parser(è§£æå™¨),Str
from langchain.schema.output_parser import StrOutputParser
# Lambdaè¡¨è¾¾å¼  ä¸€æ¬¡æ€§å‡½æ•°(ä¸ç”¨æƒ³åå­—çš„å‡½æ•°)
from langchain.schema.runnable import RunnableLambda

# embeddingæ”¯æŒçš„ä¸€ç§é»˜è®¤çš„å…è´¹çš„embeddingæ¨¡å‹ï¼Œåœ¨è¿™æ®µä»£ç é‡Œå…¶åŠŸèƒ½å·²ç»è¢«huggingfaceæ‰€é€‰ç”¨çš„æ¨¡å‹ä»£æ›¿
from chromadb.utils import embedding_functions

default_ef = embedding_functions.DefaultEmbeddingFunction()
import requests
import json
import os

# è®¾ç½®å›½å†…é•œåƒç½‘ç«™  ä¸ç”¨é­”æ³•ä¹Ÿå¯ä»¥ä¸‹è½½æ¨¡å‹
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
from langchain_huggingface import HuggingFaceEmbeddings

# å¤ç”¨ä¸”å›ºå®šçš„æç¤ºè¯æ¨¡æ¿
SYSTEM_PROMPT = '''ä½ æ˜¯ååŠ©ç”¨æˆ·å®Œæˆé—®ç­”å·¥ä½œçš„åŠ©æ‰‹ã€‚\nä½¿ç”¨ä»¥ä¸‹æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ¥å›ç­”é—®é¢˜ã€‚\næ£€ç´¢å†…å®¹ä¸ä¸€å®šå®Œå…¨æ­£ç¡®ã€‚\nå¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œé‚£å°±å›ç­”ä½ ä¸çŸ¥é“ã€‚\nå†…å®¹å¦‚ä¸‹:\n'''

'''
ä¸RagChat.pyé…å¥—çš„åç«¯ä»£ç 
åŸç‰ˆä¸ºä¸é€‚ç”¨å¤„ç†é“¾ä»…å®ç°å‡½æ•°ä¸€æ­¥æ­¥è°ƒç”¨å®Œæˆ
å‰ç«¯ä»£ç å…³äºåŸç‰ˆçš„é…å¥—ä»£ç å·²ç»è¢«åˆ é™¤

Ragchatç±»æ–¹æ³•åŒ…æ‹¬ï¼š
1.åˆå§‹åŒ–
2.å¡«å……æç¤ºè¯
3.æ–‡ç« ç‰‡æ®µæ•´åˆ
4.è·å–æ–‡ä»¶å¹¶åˆ‡å‰²(ç”±å‰ç«¯è°ƒç”¨,å› ä¸ºåç«¯ä¸ä¿ç•™æ–‡ä»¶åœ°å€)
5.MMRæœ€å¤§è¾¹ç•Œç›¸å…³ç®—æ³•
6.fastapiæ¥å£è®¿é—®
7.å¤„ç†é“¾
'''


'''
# åŸç‰ˆ----------------------------------------------------------------------
class Ragchat:
    # åˆå§‹åŒ–éœ€è¦ä½¿ç”¨çš„æ–¹æ³•åŠå‚æ•°
    def __init__(self,embedding_fn = default_ef):
        self.model = ChatOpenAI(
            model="deepseek-chat",
            openai_api_key = "sk-11",
            openai_api_base= "http://127.0.0.1:1234/v1"
        )
        # æ–‡æœ¬åˆ†å‰²å—çš„å‚æ•°åˆå§‹åŒ–
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size = 256,
            chunk_overlap = 64
        )
        # åˆ›å»ºEmbeddingäº‹ä»¶
        self.embedding_fn = embedding_fn
        
        self.embedding_hf = HuggingFaceEmbeddings(
            model_name="sentence-transformers/paraphrase-MiniLM-L3-v2",
            model_kwargs = {"device":"cuda"},
            encode_kwargs = {
                "normalize_embeddings":True
            })

        self.prompt = ChatPromptTemplate.from_messages(
            [
                ("system",SYSTEM_PROMPT),
                ("user","{question}"),
            ]
        )

    # æ‹¼æ¥å‡ºæç¤ºè¯å’Œç”¨æˆ·é—®é¢˜
    def PromptJoin(self,text,question):
        return [
                {"role":"system","content":f"{SYSTEM_PROMPT}{text}"},
                {"role":"user","content":f"{question}"},
                ]

    # è¿™æ˜¯ä¸€ä¸ªç”¨äºæ‹¼æ¥å—/æ®µè½çš„å‡½æ•°
    # ä½œç”¨æ˜¯åœ¨æ¯ä¸ªå—ä¹‹é—´åŠ ä¸Š"\n\n"å¹¶æŠŠå—æ‹¼æ¥åœ¨ä¸€èµ·æœ€åè¿”å›ä¸€ä¸ªLiteralStringæ–‡æœ¬å­—ç¬¦ä¸²
    # æ ¼å¼åŒ–
    def FormatDocs(self,docs):
        reply = ""
        for i in range(0,len(docs)):
            reply += "\n\n".join(doc.page_content for doc in docs[i])
        return reply
 
    def GetAndSplitPDF(self,file_path:str):
        docs = PyPDFLoader(file_path=file_path).load()
        # æ ¹æ®å‚æ•°ï¼Œåˆ†å‰²æ–‡æ¡£docsï¼Œåˆ†å‰²åçš„ç»“æœä¿å­˜åœ¨chunksï¼ˆå—ï¼‰ä¸­
        chunks = self.text_splitter.split_documents(docs)
        # å°†chunksä¸­æ— ç”¨çš„ä¿¡æ¯è¿‡æ»¤ï¼Œç»“æœæ›´æ–°chunks
        chunks = filter_complex_metadata(chunks)
        docs.clear()
        return chunks

    def MMRSearch(self,chunks,query):
        answer = list()
        for i in range(0,len(chunks)):
            db = Chroma.from_documents(documents=chunks[i],embedding=self.embedding_hf)
            buffer = db.max_marginal_relevance_search(query,
                                                    k=3,
                                                    fetch_k=9)
            answer.append(buffer)
            db.delete_collection
        chunks.clear()
        # print("-------------------",answer)
        return answer

    def IngestPDFToAI(self,chunks,question):
        # chunks = self.GetAndSplitPDF(file_path)
        search_answer = self.MMRSearch(chunks,question)
        prompt_messages = self.PromptJoin(self.FormatDocs(search_answer),question)
        client = OpenAI(
        api_key="sk-1",
        base_url="http://127.0.0.1:1234/v1"
        )
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=prompt_messages,
        )
        chunks = None
        search_answer.clear()
        prompt_messages.clear()
        reply = response.choices[0].message.content
        return reply

#   ------------------------------------------------------------------------------ 
'''

#   å¤„ç†é“¾+lambdaè¡¨è¾¾å¼

class Ragchat:
    def __init__(self,embedding_fn = default_ef):
        # æœ¬åœ°æ¨¡å‹client  è¯¥å±æ€§å·²åºŸå¼ƒ
        self.model = ChatOpenAI(
            model="deepseek-chat",
            openai_api_key = "sk-11",
            openai_api_base= "http://127.0.0.1:1234/v1"
        )
        
        # æ–‡ä»¶è£å‰ªæ ¼å¼
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size = 256,
            chunk_overlap = 64
        )

        # è‡ªå¸¦é»˜è®¤embedding è¯¥å±æ€§å·²åºŸå¼ƒ
        self.embedding_fn = embedding_fn

        # huggingfaceæ¨¡å‹ä¸‹è½½å¹¶è°ƒç”¨(ä¸‹è½½è‡³ç¼“å­˜  æœªæ¥å¯èƒ½ä¼šè¿›è¡Œä¼˜åŒ–  å½“å‰å‡†ç¡®ç‡ä»…50%+)
        self.embedding_hf = HuggingFaceEmbeddings(
            model_name="BAAI/bge-small-en-v1.5",
            # å…è®¸GPU
            model_kwargs = {"device":"cuda"},
            encode_kwargs = {
                # è§„èŒƒåŒ–åµŒå…¥    åŠŸèƒ½å­˜ç–‘    ç”¨äºåŒæ„å‘é‡é•¿åº¦/æ¨¡    åœ¨è¿›è¡Œå‘é‡ç›¸ä¼¼åº¦æ¯”è¾ƒæ—¶æœ‰ä¼˜åŠ¿
                "normalize_embeddings":True
            })
        
        # åˆ›å»ºå¤„ç†é“¾
        self.chain = self.CreateChain()

    def PromptJoin(self,text,question):
        return [
                {"role":"system","content":f"{SYSTEM_PROMPT}{text}"},
                {"role":"user","content":f"{question}"},
                ]

    def FormatDocs(self,docs,question):
        reply = ""
        for i in range(0,len(docs)):
            reply += "\n\n".join(doc.page_content for doc in docs[i])
        return {
            "formatted_docs":reply,
            "question":question
        }

    def GetAndSplitPDF(self,file_path:str,file_type:str):
        # print(file_type)
        # docs = PyPDFLoader(file_path=file_path).load()
        if file_type == ".pdf":
            loader = PyPDFLoader(file_path).load()          
        elif file_type == ".docx":
            loader = Docx2txtLoader(file_path).load()
        elif file_type == ".txt":
            loader = TextLoader(file_path=file_path,encoding="utf-8").load()
        else:
            return "\nä¸æ”¯æŒè¯¥æ–‡ä»¶ç±»å‹\n"
        chunks = self.text_splitter.split_documents(loader)
        chunks = filter_complex_metadata(chunks)
        chunks_global = []
        chunks_global.append(chunks)
        # print("\n\n\n\n------------------------------\n",chunks_global)
        return chunks_global

    def MMRSearch(self,chunks,query):
        # print(chunks)
        answer = list()
        for i in range(0,len(chunks)):
            db = Chroma.from_documents(documents=chunks[i],embedding=self.embedding_hf)
            buffer = db.max_marginal_relevance_search(query,
                                                    k=3,
                                                    fetch_k=9)
            answer.append(buffer)
            db.delete_collection
        return {
            "docs":answer,
            "question":query
        }
    
    def CallOpenAI(self,prompt_messages,model):
        # ä¸ºäº†è®©modelèƒ½è¢«æ­£ç¡®çš„ä¼ é€’åˆ°è¿™é‡Œï¼Œå¤§é¢ç§¯ä¿®æ”¹ä¹‹å‰çš„é€»è¾‘ï¼Œå¯èƒ½æœ‰æ›´å¥½çš„ä¼˜åŒ–æ–¹å‘
        request_inputs = {
            "model": model["model"],
            "messages": prompt_messages
        }
        response = requests.post("http://127.0.0.1:8000/ragchat",
                                 data=json.dumps(request_inputs))
        return response.json()

    # å¤„ç†é“¾ç”Ÿæˆæ–¹æ³•
    def CreateChain(self):
        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
        # éå¸¸éå¸¸éå¸¸éå¸¸é‡è¦é‡è¦é‡è¦é‡è¦
        # è¿™ä¸ªå‚æ•°ä¼ è¿›æ¥çš„ï¼Œå°±æ˜¯åŸå§‹çš„å€¼ï¼Œä¼ è¿›çš„å‚æ•°ä¿å­˜åœ¨å“ªï¼Œä½¿ç”¨çš„å‚æ•°å°±åœ¨å“ª
        # ç›´ç™½ç‚¹    åœ¨å‡½æ•°é‡ŒæŠŠä¼ å‚æ”¹äº†ï¼Œé‚£å°±æ˜¯çœŸçš„æ”¹äº†ğŸ˜¡æœ‰ç—…å§è¿åƒå¸¦æ‹¿
        chain_input = {
            "chunks":RunnablePassthrough(),
            "question":RunnablePassthrough(),
            "model":RunnablePassthrough()
        }
        
        # lambdaè¡¨è¾¾å¼ :å‰çš„xè¡¨ç¤ºå‚æ•°   :åçš„è¯­å¥è¡¨ç¤ºreturnå†…å®¹

        # mmrç®—æ³•è·å¾—æœ€ç›¸è¿‘çš„å†…å®¹
        mmr_search = RunnableLambda(lambda x:self.MMRSearch(x["chunks"],x["question"]))

        # å°†ç›¸è¿‘å†…å®¹ç»„åˆåœ¨ä¸€èµ·æ–¹ä¾¿æä¾›ç»™æ¨¡å‹
        format_docs = RunnableLambda(lambda x:self.FormatDocs(x["docs"],x["question"]))

        # å°†ç»„åˆå¥½çš„è¯å’Œæœ€å¼€å§‹ä¼ å…¥çš„questionæ‹¼æ¥ä¸ºæç¤ºè¯
        # é“¾å¼å¤„ç†æµç¨‹ä¼šä¿å­˜ä¼ å…¥çš„å‚æ•°ï¼Œå› æ­¤åœ¨è¿™é‡Œè™½ç„¶ä¸Šä¸€ä¸ªæµç¨‹ç»“æŸåæ²¡æœ‰ä¼ å‡ºquestion
        # ä»ç„¶å¯ä»¥é€šè¿‡RunnablePassthrough()è·å¾—ä¸€å¼€å§‹ä¼ å…¥çš„é—®é¢˜
        join_prompt = RunnableLambda(lambda x:self.PromptJoin(x["formatted_docs"],x["question"]))

        # å°†æç¤ºè¯å‘é€ç»™AIç­‰å¾…å›å¤
        call_openai = RunnableLambda(lambda x:self.CallOpenAI(x["joined_docs"],x["model"]))

        # é“¾å¼å¤„ç†æµç¨‹  æµç¨‹è·å¾—çš„å‚æ•°ä¸ºä¸Šä¸€ä¸ªæµç¨‹ä¼ å‡ºçš„å‚æ•°

        return (
            chain_input
            | {
                "joined_docs": mmr_search | format_docs | join_prompt,
                "model":RunnablePassthrough()
            }
            | call_openai
        ) 
    
    # å¯åŠ¨æ–¹å¼  IngestQueryç¿»è¯‘ï¼šé‡‡æ ·æŸ¥è¯¢
    def IngestQuery(self,chunks,question,model):
        # print("=============================",chunks)
        # è¿™æ®µä»£ç å¤§æ¦‚ç‡æ˜¯ä¸ä¼šè§¦å‘çš„
        if not chunks:
            return "è¯·ä¸Šä¼ è‡³å°‘ä¸€ä¸ªæ–‡ä»¶æˆ–ç­‰å¾…æ–‡ä»¶è¯»å–å®Œæ¯•ï¼"
        elif not question:
            return "è¯·è¾“å…¥æœ‰æ•ˆå†…å®¹ï¼"
        # å¤„ç†é“¾çš„ç”Ÿæˆå’Œè¿è¡Œæ˜¯åˆ†ç¦»å¼€æ¥çš„ï¼Œå³ä½¿ç”Ÿæˆå¥½äº†å¤„ç†é“¾æ²¡æœ‰ä½¿ç”¨invoke()è¿›è¡Œä¼ å‚
        # é‚£ä¹ˆå¤„ç†é“¾ä¹Ÿä¸ä¼šå¯åŠ¨ï¼Œä»£ç ä¹Ÿä¸ä¼šç”Ÿæ•ˆ
        return self.chain.invoke({
            "chunks":chunks,
            "question":question,
            "model":model,
        })
    

# file_path = "C:/Users/17937/Desktop/ç¬¬åå…­å±Šæœåˆ›å¤§èµ›ã€Šå‚èµ›æ‰‹å†Œã€‹ã€ã€Šèµ›é¢˜æ‰‹å†Œã€‹ç­‰ç³»åˆ—ææ–™/03-ç¬¬åå…­å±Šä¸­å›½å¤§å­¦ç”ŸæœåŠ¡å¤–åŒ…åˆ›æ–°åˆ›ä¸šå¤§èµ›å‚èµ›æ‰¿è¯ºä¹¦.pdf"
# Test = Ragchat()
# chunks = []
# chunks.append(Test.GetAndSplitPDF(file_path))
# question = "æ–‡ç« çš„ä½œè€…æ˜¯è°ï¼Ÿ"
# print(Test.IngestQuery(chunks,question,"local_deepseek"))